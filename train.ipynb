{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "\n",
    "from melbanks import LogMelFilterBanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = Path('data/SpeechCommands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATASET_DIR.exists():\n",
    "     SPEECHCOMMANDS('data', download=True)\n",
    "     !rm data/speech_commands_v0.02.tar.gz\n",
    "     for label_dir in Path(DATASET_DIR, 'speech_commands_v0.02').glob('*'):\n",
    "          if label_dir.is_dir() and label_dir.name not in ['yes', 'no']:\n",
    "               shutil.rmtree(label_dir)\n",
    "     !sed -i '/no\\/\\|yes\\//!d' data/SpeechCommands/speech_commands_v0.02/testing_list.txt\n",
    "     !sed -i '/no\\/\\|yes\\//!d' data/SpeechCommands/speech_commands_v0.02/validation_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SPEECHCOMMANDS('data', subset='training')\n",
    "val_dataset = SPEECHCOMMANDS('data', subset='validation')\n",
    "test_dataset = SPEECHCOMMANDS('data', subset='testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{torch.Size([1, 80, 43]),\n",
       " torch.Size([1, 80, 47]),\n",
       " torch.Size([1, 80, 52]),\n",
       " torch.Size([1, 80, 56]),\n",
       " torch.Size([1, 80, 60]),\n",
       " torch.Size([1, 80, 61]),\n",
       " torch.Size([1, 80, 65]),\n",
       " torch.Size([1, 80, 66]),\n",
       " torch.Size([1, 80, 69]),\n",
       " torch.Size([1, 80, 70]),\n",
       " torch.Size([1, 80, 73]),\n",
       " torch.Size([1, 80, 75]),\n",
       " torch.Size([1, 80, 76]),\n",
       " torch.Size([1, 80, 77]),\n",
       " torch.Size([1, 80, 79]),\n",
       " torch.Size([1, 80, 82]),\n",
       " torch.Size([1, 80, 84]),\n",
       " torch.Size([1, 80, 86]),\n",
       " torch.Size([1, 80, 89]),\n",
       " torch.Size([1, 80, 90]),\n",
       " torch.Size([1, 80, 93]),\n",
       " torch.Size([1, 80, 94]),\n",
       " torch.Size([1, 80, 98]),\n",
       " torch.Size([1, 80, 99]),\n",
       " torch.Size([1, 80, 101])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(map(lambda x: mel(x[0]).shape, val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_loader = DataLoader(val_dataset, collate_fn=)\n",
    "test_loader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=2, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogMelFilterBanks()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel = LogMelFilterBanks()\n",
    "mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M5(\n",
      "  (conv1): Conv1d(1, 32, kernel_size=(80,), stride=(16,))\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "Number of parameters: 24770\n"
     ]
    }
   ],
   "source": [
    "model = M5()\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, train_loader, test_loader, classes,\n",
    "                 metric_name,\n",
    "                 project_name='itmo-dsp',\n",
    "                 checkpoint_folder='./models'):\n",
    "        self.project_name = project_name\n",
    "        self.checkpoint_folder = checkpoint_folder\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.true_val_labels = np.hstack([x[1].numpy() for x in test_loader])\n",
    "        self.samples = list(map(lambda img: (img - img.min()) / (img.max() - img.min()),\n",
    "                                next(iter(test_loader))[0][:10].permute((0, 2, 3, 1)).numpy()))\n",
    "        self.best_metric = 0.0\n",
    "        self.classes = classes\n",
    "        self.test_dataset_size = len(self.test_loader.dataset)\n",
    "        self.train_dataset_size = len(self.train_loader.dataset)\n",
    "        self.metric_name = metric_name\n",
    "\n",
    "    def validate(self, model, device):\n",
    "        model.eval()\n",
    "        accuracy = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                accuracy += (preds == labels).sum().detach().cpu() / self.test_dataset_size\n",
    "\n",
    "        metrics = {\n",
    "            'val_acc': accuracy,\n",
    "        }\n",
    "        return metrics\n",
    "        \n",
    "    def save_model_locally(self, model):\n",
    "        torch.save(model.state_dict(), self.best_model_path)\n",
    "\n",
    "    def track_progress(self, metrics, patience, trial, epoch, epochs_without_improvement):\n",
    "        if metrics[self.metric_name] > self.best_metric:\n",
    "            epochs_without_improvement = 0\n",
    "            self.best_metric = metrics[self.metric_name]\n",
    "            self.best_model_path = Path(self.checkpoint_folder) / f'{self.metric_name}_{self.metric_name:.4f}.pth'\n",
    "        else:\n",
    "            if epochs_without_improvement > patience:\n",
    "                print('Early stopping')\n",
    "                raise optuna.TrialPruned()\n",
    "            epochs_without_improvement += 1\n",
    "        if trial:\n",
    "            trial.report(metrics[self.metric_name], epoch)\n",
    "            if trial.should_prune():\n",
    "                print('[OPTUNA] Run pruned')\n",
    "                raise optuna.TrialPruned()\n",
    "        return epochs_without_improvement\n",
    "    \n",
    "    def log_metrics(self, metrics, epoch, progress_bar, n_epochs):\n",
    "        progress_bar.set_postfix(metrics)\n",
    "        print(f'Epoch {epoch + 1}/{n_epochs}:')\n",
    "        print(f'Best {self.metric_name}: {self.best_metric:.4f}')\n",
    "        \n",
    "    def train_epoch(self, model, optimizer, loss_fn, device):\n",
    "        model.train()\n",
    "        loss = 0\n",
    "        \n",
    "        for inputs, labels in self.train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss += loss.item().detach().cpu() / self.train_dataset_size\n",
    "\n",
    "        metrics = {\n",
    "            'train_loss': loss,\n",
    "        }\n",
    "        \n",
    "        return model, metrics\n",
    "\n",
    "    def train(self, model, loss_fn, optim_class, optim_args, device, \n",
    "              n_epochs=100, patience=10, trial=None):\n",
    "        run = wandb.init(project=self.project_name, name=f'{model.__class__.__name__}_{optim_class.__name__}lr{np.round(optim_args[\"lr\"], 5)}')\n",
    "        optimizer = optim_class(model.parameters(), **optim_args)\n",
    "        model = model.to(device)\n",
    "        self.best_model_path = None\n",
    "        progress_bar = tqdm(range(n_epochs), desc='Training', leave=True)\n",
    "        epochs_without_improvement = 0\n",
    "        try:\n",
    "            for epoch in progress_bar:\n",
    "                model, train_metrics = self.train_epoch(model, optimizer, loss_fn, device)\n",
    "                val_metrics = self.validate(model, device)\n",
    "                metrics = {**train_metrics, **val_metrics, 'epoch': epoch}\n",
    "                epochs_without_improvement = self.track_progress(metrics, patience, trial, epoch, epochs_without_improvement)\n",
    "                if epochs_without_improvement == 0:\n",
    "                    self.save_model_locally(model, optimizer, metrics)\n",
    "                self.log_metrics(run, metrics, epoch, progress_bar, n_epochs)\n",
    "                clear_output(wait=True)\n",
    "                plt.show()\n",
    "        except (Exception, KeyboardInterrupt) as e:\n",
    "            print(traceback.format_exc())\n",
    "        finally:\n",
    "            progress_bar.close()\n",
    "            wandb.finish()\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trainer, trial, model_class, loss_fn, device, n_epochs=100):\n",
    "    model = model_class()\n",
    "    optimizer_class = optim.Adam\n",
    "    lr = trial.suggest_float('lr', low=1e-5, high=1e-1, log=True)\n",
    "    trainer.load_data(train_loader=DataLoader(train_dataset, batch_size=batch_sz, shuffle=True),\n",
    "                      anomaly_loader=DataLoader(anomaly_dataset, batch_size=len(anomaly_dataset)))\n",
    "    run_config = {\n",
    "        'use_batchnorm': use_batch_norm,\n",
    "        'optimizer': optimizer_name,\n",
    "        'batch_sz': batch_sz}\n",
    "    if activation_func_name == 'LeakyReLU':\n",
    "        run_config['negative_slope'] = negative_slope\n",
    "    trainer.train(\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optim_class=optimizer_class,\n",
    "        optim_args={'lr': lr},\n",
    "        device=device,\n",
    "        n_epochs=n_epochs,\n",
    "        trial=trial,\n",
    "        run_config=run_config\n",
    "    )\n",
    "    return trainer.best_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ModelTrainer(train_loader, test_loader, classes)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trained_model = trainer.optimize_lr(\n",
    "    model_class=M5(),\n",
    "    optim_class=optim.Adam,\n",
    "    loss_fn=loss_fn,\n",
    "    n_epochs=100,\n",
    "    n_trials=20\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
