{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install optuna"
      ],
      "metadata": {
        "id": "v5f20LembBut",
        "outputId": "c7c1820d-d4f0-48bf-875f-5880b507e3e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.39)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.1-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.15.1 colorlog-6.9.0 optuna-4.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Mh5wI7myY_7V"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import traceback\n",
        "from pathlib import Path\n",
        "\n",
        "import wandb\n",
        "import optuna\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn, optim\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "from torchaudio.datasets.speechcommands import SPEECHCOMMANDS, URL, FOLDER_IN_ARCHIVE\n",
        "\n",
        "from melbanks import LogMelFilterBanks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VEpLRnsQY_7Y"
      },
      "outputs": [],
      "source": [
        "DATASET_DIR = Path('data')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL_ENC = {'yes': 1, 'no': 0}"
      ],
      "metadata": {
        "id": "DOiGon_leKEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bpJ7T4HuY_7Y",
        "outputId": "0f5579b6-4971-4869-a9a5-0e4bf77451ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/speech_commands_v0.02.tar.gz.2499796eed7c4743a4f875592e6f2c15.partial'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-195fad2a6cdc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mDATASET_DIR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m      \u001b[0mSPEECHCOMMANDS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m      \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm data/speech_commands_v0.02.tar.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m      \u001b[0;32mfor\u001b[0m \u001b[0mlabel_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'speech_commands_v0.02'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mlabel_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlabel_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'yes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'no'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchaudio/datasets/speechcommands.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, url, folder_in_archive, download, subset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mchecksum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CHECKSUMS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mdownload_url_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchecksum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0m_extract_tar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mtmp_dst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdst\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".partial\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/speech_commands_v0.02.tar.gz.2499796eed7c4743a4f875592e6f2c15.partial'"
          ]
        }
      ],
      "source": [
        "if not DATASET_DIR.exists():\n",
        "     SPEECHCOMMANDS(DATASET_DIR, download=True)\n",
        "     DATASET_DIR = DATASET_DIR / FOLDER_IN_ARCHIVE / URL\n",
        "     for label_dir in Path(DATASET_DIR).glob('*'):\n",
        "          if label_dir.is_dir() and label_dir.name not in LABEL_ENC.keys():\n",
        "               shutil.rmtree(label_dir)\n",
        "else:\n",
        "      DATASET_DIR = DATASET_DIR / FOLDER_IN_ARCHIVE / URL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm data/speech_commands_v0.02.tar.gz\n",
        "!sed -i '/no\\/\\|yes\\//!d' data/SpeechCommands/speech_commands_v0.02/testing_list.txt\n",
        "!sed -i '/no\\/\\|yes\\//!d' data/SpeechCommands/speech_commands_v0.02/validation_list.txt"
      ],
      "metadata": {
        "id": "LciCfPS8ecnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qgHshGQY_7Z"
      },
      "outputs": [],
      "source": [
        "train_dataset = SPEECHCOMMANDS('data', subset='training')\n",
        "val_dataset = SPEECHCOMMANDS('data', subset='validation')\n",
        "test_dataset = SPEECHCOMMANDS('data', subset='testing')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "megpZWUgY_7Z",
        "outputId": "a440ae01-f4f6-433a-ec44-37c087561168"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7s6aJ4GY_7a",
        "outputId": "e8a45492-bd98-4f5d-b14a-ed8a24fa99ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{torch.Size([1, 80, 43]),\n",
              " torch.Size([1, 80, 47]),\n",
              " torch.Size([1, 80, 52]),\n",
              " torch.Size([1, 80, 56]),\n",
              " torch.Size([1, 80, 60]),\n",
              " torch.Size([1, 80, 61]),\n",
              " torch.Size([1, 80, 65]),\n",
              " torch.Size([1, 80, 66]),\n",
              " torch.Size([1, 80, 69]),\n",
              " torch.Size([1, 80, 70]),\n",
              " torch.Size([1, 80, 73]),\n",
              " torch.Size([1, 80, 75]),\n",
              " torch.Size([1, 80, 76]),\n",
              " torch.Size([1, 80, 77]),\n",
              " torch.Size([1, 80, 79]),\n",
              " torch.Size([1, 80, 82]),\n",
              " torch.Size([1, 80, 84]),\n",
              " torch.Size([1, 80, 86]),\n",
              " torch.Size([1, 80, 89]),\n",
              " torch.Size([1, 80, 90]),\n",
              " torch.Size([1, 80, 93]),\n",
              " torch.Size([1, 80, 94]),\n",
              " torch.Size([1, 80, 98]),\n",
              " torch.Size([1, 80, 99]),\n",
              " torch.Size([1, 80, 101])}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set(map(lambda x: mel(x[0]).shape, val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojEeJvkxY_7a"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHoXTJNxY_7b"
      },
      "outputs": [],
      "source": [
        "\n",
        "val_loader = DataLoader(val_dataset, collate_fn=)\n",
        "test_loader = DataLoader(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB7KNSoRY_7d"
      },
      "outputs": [],
      "source": [
        "class M5(nn.Module):\n",
        "    def __init__(self, n_input=1, n_output=2, stride=16, n_channel=32):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = LogMelFilterBanks(n_mels=n_mels)\n",
        "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
        "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
        "        self.pool1 = nn.MaxPool1d(4)\n",
        "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
        "        self.pool2 = nn.MaxPool1d(4)\n",
        "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
        "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
        "        self.pool3 = nn.MaxPool1d(4)\n",
        "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
        "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
        "        self.pool4 = nn.MaxPool1d(4)\n",
        "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(self.bn1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(self.bn2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(self.bn3(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = F.relu(self.bn4(x))\n",
        "        x = self.pool4(x)\n",
        "        x = F.avg_pool1d(x, x.shape[-1])\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1IQzT4EY_7e",
        "outputId": "dc4af934-0f9c-4947-f682-91bb459de15e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogMelFilterBanks()"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mel = LogMelFilterBanks()\n",
        "mel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFoh-ED7Y_7f",
        "outputId": "2b7d3767-7322-4488-9ee4-a429f63b1a5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M5(\n",
            "  (conv1): Conv1d(1, 32, kernel_size=(80,), stride=(16,))\n",
            "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
            "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
            "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
            "  (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "Number of parameters: 24770\n"
          ]
        }
      ],
      "source": [
        "model = M5()\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "n = count_parameters(model)\n",
        "print(\"Number of parameters: %s\" % n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc71DlrIY_7f"
      },
      "outputs": [],
      "source": [
        "class ModelTrainer:\n",
        "    def __init__(self, train_loader, test_loader, classes,\n",
        "                 metric_name,\n",
        "                 project_name='itmo-dsp',\n",
        "                 checkpoint_folder='./models'):\n",
        "        self.project_name = project_name\n",
        "        self.checkpoint_folder = checkpoint_folder\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.true_val_labels = np.hstack([x[1].numpy() for x in test_loader])\n",
        "        self.samples = list(map(lambda img: (img - img.min()) / (img.max() - img.min()),\n",
        "                                next(iter(test_loader))[0][:10].permute((0, 2, 3, 1)).numpy()))\n",
        "        self.best_metric = 0.0\n",
        "        self.classes = classes\n",
        "        self.test_dataset_size = len(self.test_loader.dataset)\n",
        "        self.train_dataset_size = len(self.train_loader.dataset)\n",
        "        self.metric_name = metric_name\n",
        "\n",
        "    def validate(self, model, device):\n",
        "        model.eval()\n",
        "        accuracy = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in self.test_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                accuracy += (preds == labels).sum().detach().cpu() / self.test_dataset_size\n",
        "\n",
        "        metrics = {\n",
        "            'val_acc': accuracy,\n",
        "        }\n",
        "        return metrics\n",
        "\n",
        "    def save_model_locally(self, model):\n",
        "        torch.save(model.state_dict(), self.best_model_path)\n",
        "\n",
        "    def track_progress(self, metrics, patience, trial, epoch, epochs_without_improvement):\n",
        "        if metrics[self.metric_name] > self.best_metric:\n",
        "            epochs_without_improvement = 0\n",
        "            self.best_metric = metrics[self.metric_name]\n",
        "            self.best_model_path = Path(self.checkpoint_folder) / f'{self.metric_name}_{self.metric_name:.4f}.pth'\n",
        "        else:\n",
        "            if epochs_without_improvement > patience:\n",
        "                print('Early stopping')\n",
        "                raise optuna.TrialPruned()\n",
        "            epochs_without_improvement += 1\n",
        "        if trial:\n",
        "            trial.report(metrics[self.metric_name], epoch)\n",
        "            if trial.should_prune():\n",
        "                print('[OPTUNA] Run pruned')\n",
        "                raise optuna.TrialPruned()\n",
        "        return epochs_without_improvement\n",
        "\n",
        "    def log_metrics(self, metrics, epoch, progress_bar, n_epochs):\n",
        "        progress_bar.set_postfix(metrics)\n",
        "        print(f'Epoch {epoch + 1}/{n_epochs}:')\n",
        "        print(f'Best {self.metric_name}: {self.best_metric:.4f}')\n",
        "\n",
        "    def train_epoch(self, model, optimizer, loss_fn, device):\n",
        "        model.train()\n",
        "        loss = 0\n",
        "\n",
        "        for inputs, labels in self.train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss += loss.item().detach().cpu() / self.train_dataset_size\n",
        "\n",
        "        metrics = {\n",
        "            'train_loss': loss,\n",
        "        }\n",
        "\n",
        "        return model, metrics\n",
        "\n",
        "    def train(self, model, loss_fn, optim_class, optim_args, device,\n",
        "              n_epochs=100, patience=10, trial=None):\n",
        "        run = wandb.init(project=self.project_name, name=f'{model.__class__.__name__}_{optim_class.__name__}lr{np.round(optim_args[\"lr\"], 5)}')\n",
        "        optimizer = optim_class(model.parameters(), **optim_args)\n",
        "        model = model.to(device)\n",
        "        self.best_model_path = None\n",
        "        progress_bar = tqdm(range(n_epochs), desc='Training', leave=True)\n",
        "        epochs_without_improvement = 0\n",
        "        try:\n",
        "            for epoch in progress_bar:\n",
        "                model, train_metrics = self.train_epoch(model, optimizer, loss_fn, device)\n",
        "                val_metrics = self.validate(model, device)\n",
        "                metrics = {**train_metrics, **val_metrics, 'epoch': epoch}\n",
        "                epochs_without_improvement = self.track_progress(metrics, patience, trial, epoch, epochs_without_improvement)\n",
        "                if epochs_without_improvement == 0:\n",
        "                    self.save_model_locally(model, optimizer, metrics)\n",
        "                self.log_metrics(run, metrics, epoch, progress_bar, n_epochs)\n",
        "                clear_output(wait=True)\n",
        "                plt.show()\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            print(traceback.format_exc())\n",
        "        finally:\n",
        "            progress_bar.close()\n",
        "            wandb.finish()\n",
        "        return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMulTp7rY_7g"
      },
      "outputs": [],
      "source": [
        "def objective(trainer, trial, model_class, loss_fn, device, n_epochs=100):\n",
        "    model = model_class()\n",
        "    optimizer_class = optim.Adam\n",
        "    lr = trial.suggest_float('lr', low=1e-5, high=1e-1, log=True)\n",
        "    trainer.load_data(train_loader=DataLoader(train_dataset, batch_size=batch_sz, shuffle=True),\n",
        "                      anomaly_loader=DataLoader(anomaly_dataset, batch_size=len(anomaly_dataset)))\n",
        "    run_config = {\n",
        "        'use_batchnorm': use_batch_norm,\n",
        "        'optimizer': optimizer_name,\n",
        "        'batch_sz': batch_sz}\n",
        "    if activation_func_name == 'LeakyReLU':\n",
        "        run_config['negative_slope'] = negative_slope\n",
        "    trainer.train(\n",
        "        model=model,\n",
        "        loss_fn=loss_fn,\n",
        "        optim_class=optimizer_class,\n",
        "        optim_args={'lr': lr},\n",
        "        device=device,\n",
        "        n_epochs=n_epochs,\n",
        "        trial=trial,\n",
        "        run_config=run_config\n",
        "    )\n",
        "    return trainer.best_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJsguErrY_7g"
      },
      "outputs": [],
      "source": [
        "trainer = ModelTrainer(train_loader, test_loader, classes)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2RfXJNpY_7g"
      },
      "outputs": [],
      "source": [
        "\n",
        "trained_model = trainer.optimize_lr(\n",
        "    model_class=M5(),\n",
        "    optim_class=optim.Adam,\n",
        "    loss_fn=loss_fn,\n",
        "    n_epochs=100,\n",
        "    n_trials=20\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}